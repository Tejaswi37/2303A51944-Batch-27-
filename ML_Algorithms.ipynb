{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgmrVOwjjukVpNvTkGD1AG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejaswi37/2303A51944-Batch-27-/blob/main/ML_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ensemble clustering pipeline for Debt Burden & Risk Segmentation\n",
        "\n",
        "This script performs:\n",
        "- Data loading and preprocessing\n",
        "- Feature engineering and outlier removal\n",
        "- Multiple base clusterings (KMeans, Agglomerative, GMM, DBSCAN)\n",
        "- Build a co-association (consensus) matrix from base clusterings\n",
        "- Apply Spectral Clustering on the consensus matrix to obtain final ensemble clusters\n",
        "- Evaluate clusters with Silhouette, Davies-Bouldin, Calinski-Harabasz indices\n",
        "- Profile clusters and save results\n",
        "\n",
        "Usage: edit `DATA_PATH` and run.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIG\n",
        "# -----------------------------\n",
        "DATA_PATH = \"/content/synthetic_personal_finance_dataset (1) (1).csv\"\n",
        "OUTPUT_DIR = \"/mnt/data/ensemble_clustering_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "BASE_K_RANGE = range(2, 7)  # candidate k for base KMeans/GMM\n",
        "FINAL_K_CANDIDATES = range(2, 8)  # candidate final cluster numbers\n",
        "\n",
        "# -----------------------------\n",
        "# HELPERS\n",
        "# -----------------------------\n",
        "\n",
        "def load_data(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Data file not found at {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    print(f\"Loaded data with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def select_features(df):\n",
        "    # Use the features suggested in the problem statement\n",
        "    features = [\n",
        "        \"monthly_income_usd\",\n",
        "        \"monthly_expenses_usd\",\n",
        "        \"savings_usd\",\n",
        "        \"debt_to_income_ratio\",\n",
        "        \"savings_to_income_ratio\",\n",
        "    ]\n",
        "    missing = [f for f in features if f not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "    return df[features].copy()\n",
        "\n",
        "\n",
        "def preprocess(df_feat):\n",
        "    # Impute missing values with median (robust) and remove impossible values\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    X = imputer.fit_transform(df_feat)\n",
        "    X = pd.DataFrame(X, columns=df_feat.columns)\n",
        "\n",
        "    # Replace negative or zero incomes/ratios where nonsensical (if they exist) with small positive\n",
        "    X[\"monthly_income_usd\"] = X[\"monthly_income_usd\"].apply(lambda v: max(v, 1.0))\n",
        "    X[\"monthly_expenses_usd\"] = X[\"monthly_expenses_usd\"].apply(lambda v: max(v, 0.0))\n",
        "    X[\"savings_usd\"] = X[\"savings_usd\"].apply(lambda v: max(v, 0.0))\n",
        "\n",
        "    # Additional feature: expenses_to_income_ratio\n",
        "    X[\"expenses_to_income_ratio\"] = X[\"monthly_expenses_usd\"] / X[\"monthly_income_usd\"]\n",
        "\n",
        "    # Cap extreme ratios to a reasonable bound to avoid infinities\n",
        "    X[\"expenses_to_income_ratio\"] = X[\"expenses_to_income_ratio\"].clip(0, 10)\n",
        "    X[\"debt_to_income_ratio\"] = pd.Series(X[\"debt_to_income_ratio\"]).clip(0, 5)\n",
        "    X[\"savings_to_income_ratio\"] = pd.Series(X[\"savings_to_income_ratio\"]).clip(0, 20)\n",
        "\n",
        "    # Outlier removal via IsolationForest (flag outliers but keep them in a separate column)\n",
        "    iso = IsolationForest(n_estimators=200, contamination=0.02, random_state=RANDOM_STATE)\n",
        "    outlier_flag = iso.fit_predict(X)\n",
        "    X[\"is_outlier\"] = (outlier_flag == -1).astype(int)\n",
        "\n",
        "    # Scale features (RobustScaler to limit outlier influence)\n",
        "    scaler = RobustScaler()\n",
        "    features_to_scale = [c for c in X.columns if c != \"is_outlier\"]\n",
        "    X_scaled = scaler.fit_transform(X[features_to_scale])\n",
        "\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=features_to_scale)\n",
        "    X_scaled[\"is_outlier\"] = X[\"is_outlier\"].values\n",
        "\n",
        "    return X_scaled, scaler\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# BASE CLUSTERINGS\n",
        "# -----------------------------\n",
        "\n",
        "def generate_base_clusterings(X, random_state=RANDOM_STATE):\n",
        "    \"\"\"\n",
        "    Create a list of base clusterings (as label arrays). We vary algorithms and hyperparameters\n",
        "    to create a diverse ensemble.\n",
        "    \"\"\"\n",
        "    clusterings = []\n",
        "\n",
        "    # 1) KMeans with different k\n",
        "    for k in BASE_K_RANGE:\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=20)\n",
        "        labels = km.fit_predict(X)\n",
        "        clusterings.append((f\"kmeans_k{k}\", labels))\n",
        "\n",
        "    # 2) Gaussian Mixture with different k\n",
        "    for k in BASE_K_RANGE:\n",
        "        gmm = GaussianMixture(n_components=k, random_state=random_state)\n",
        "        labels = gmm.fit_predict(X)\n",
        "        clusterings.append((f\"gmm_k{k}\", labels))\n",
        "\n",
        "    # 3) Agglomerative (ward) with different k\n",
        "    for k in BASE_K_RANGE:\n",
        "        agg = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "        labels = agg.fit_predict(X)\n",
        "        clusterings.append((f\"agg_k{k}\", labels))\n",
        "\n",
        "    # 4) DBSCAN - density-based for outlier detection\n",
        "    # We'll try a few eps values scaled for our scaled data\n",
        "    for eps in [0.3, 0.5, 0.7]:\n",
        "        db = DBSCAN(eps=eps, min_samples=10)\n",
        "        labels = db.fit_predict(X)\n",
        "        clusterings.append((f\"dbscan_eps{eps}\", labels))\n",
        "\n",
        "    print(f\"Generated {len(clusterings)} base clusterings\")\n",
        "    return clusterings\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CONSENSUS (CO-ASSOCIATION) MATRIX\n",
        "# -----------------------------\n",
        "\n",
        "def build_coassociation_matrix(clusterings, n_samples):\n",
        "    \"\"\"\n",
        "    Build co-association matrix A where A[i,j] is fraction of clusterings that placed i and j together.\n",
        "    Treat DBSCAN noise points (label -1) as their own unique cluster assignment.\n",
        "    \"\"\"\n",
        "    co_assoc = np.zeros((n_samples, n_samples), dtype=float)\n",
        "    m = len(clusterings)\n",
        "    for name, labels in clusterings:\n",
        "        # Convert -1 labels to unique labels by offsetting them to big unique values per sample\n",
        "        lbl = np.array(labels)\n",
        "        # For DBSCAN noise points (-1), we keep as unique by mapping each -1 occurrence to unique label id\n",
        "        noise_indices = np.where(lbl == -1)[0]\n",
        "        if len(noise_indices) > 0:\n",
        "            # create a copy to avoid modifying original\n",
        "            lbl = lbl.copy()\n",
        "            for idx in noise_indices:\n",
        "                lbl[idx] = -100000 - idx\n",
        "\n",
        "        # build pairwise equality\n",
        "        eq = (lbl[:, None] == lbl[None, :]).astype(float)\n",
        "        co_assoc += eq\n",
        "\n",
        "    co_assoc /= float(m)\n",
        "    return co_assoc\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ENSEMBLE (SPECTRAL ON CO-ASSOC)\n",
        "# -----------------------------\n",
        "\n",
        "def spectral_consensus_clustering(co_assoc, k):\n",
        "    \"\"\"\n",
        "    Apply spectral clustering on the co-association matrix treated as affinity.\n",
        "    \"\"\"\n",
        "    # Ensure matrix is symmetric and in [0,1]\n",
        "    A = (co_assoc + co_assoc.T) / 2.0\n",
        "    # Numerical stability\n",
        "    A = np.nan_to_num(A)\n",
        "    sc = SpectralClustering(n_clusters=k, affinity=\"precomputed\", random_state=RANDOM_STATE)\n",
        "    labels = sc.fit_predict(A)\n",
        "    return labels\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# EVALUATION\n",
        "# -----------------------------\n",
        "\n",
        "def evaluate_clustering(X, labels):\n",
        "    # filter out cases where only one cluster exists\n",
        "    if len(np.unique(labels)) <= 1:\n",
        "        return {\n",
        "            \"silhouette\": -1,\n",
        "            \"davies_bouldin\": np.inf,\n",
        "            \"calinski_harabasz\": -1,\n",
        "        }\n",
        "    metrics = {\n",
        "        \"silhouette\": silhouette_score(X, labels),\n",
        "        \"davies_bouldin\": davies_bouldin_score(X, labels),\n",
        "        \"calinski_harabasz\": calinski_harabasz_score(X, labels),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLUSTER PROFILING\n",
        "# -----------------------------\n",
        "\n",
        "def profile_clusters(original_df, X_scaled_df, labels, output_prefix):\n",
        "    dfp = original_df.copy()\n",
        "    dfp[\"ensemble_cluster\"] = labels\n",
        "    summary = dfp.groupby(\"ensemble_cluster\").agg(\n",
        "        count=(\"ensemble_cluster\", \"count\"),\n",
        "        mean_income=(\"monthly_income_usd\", \"mean\"),\n",
        "        median_income=(\"monthly_income_usd\", \"median\"),\n",
        "        mean_expenses=(\"monthly_expenses_usd\", \"mean\"),\n",
        "        mean_savings=(\"savings_usd\", \"mean\"),\n",
        "        mean_dti=(\"debt_to_income_ratio\", \"mean\"),\n",
        "        mean_savings_to_income=(\"savings_to_income_ratio\", \"mean\"),\n",
        "    ).reset_index()\n",
        "    summary = summary.sort_values(\"count\", ascending=False)\n",
        "    summary.to_csv(os.path.join(OUTPUT_DIR, f\"{output_prefix}_cluster_profiles.csv\"), index=False)\n",
        "    print(\"Saved cluster profiles to CSV\")\n",
        "    return summary\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# VISUALIZATION\n",
        "# -----------------------------\n",
        "\n",
        "def visualize_embedding(X, labels, title, filename):\n",
        "    pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "    emb = pca.fit_transform(X)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(emb[:, 0], emb[:, 1], c=labels, s=10)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN PIPELINE\n",
        "# -----------------------------\n",
        "\n",
        "def main():\n",
        "    df = load_data(DATA_PATH)\n",
        "\n",
        "    # Keep original for profiling\n",
        "    df_original = df.copy()\n",
        "\n",
        "    # select features and preprocess\n",
        "    df_feat = select_features(df)\n",
        "    X_scaled_df, scaler = preprocess(df_feat)\n",
        "\n",
        "    # Prepare data matrix for clustering (exclude is_outlier for clustering; keep it for profiling)\n",
        "    X_for_clustering = X_scaled_df.drop(columns=[\"is_outlier\"]).values\n",
        "\n",
        "    # generate base clusterings\n",
        "    base_clusterings = generate_base_clusterings(X_for_clustering)\n",
        "\n",
        "    # Build co-association matrix\n",
        "    co_assoc = build_coassociation_matrix(base_clusterings, n_samples=X_for_clustering.shape[0])\n",
        "    np.save(os.path.join(OUTPUT_DIR, \"co_association_matrix.npy\"), co_assoc)\n",
        "    print(\"Saved co-association matrix\")\n",
        "\n",
        "    # Find best final k by trying several and using silhouette as primary metric\n",
        "    best_k = None\n",
        "    best_score = -999\n",
        "    best_labels = None\n",
        "    evaluations = []\n",
        "    for k in FINAL_K_CANDIDATES:\n",
        "        labels_k = spectral_consensus_clustering(co_assoc, k)\n",
        "        metrics = evaluate_clustering(X_for_clustering, labels_k)\n",
        "        evaluations.append((k, metrics))\n",
        "        print(f\"k={k}: silhouette={metrics['silhouette']:.4f}, db={metrics['davies_bouldin']:.4f}, ch={metrics['calinski_harabasz']:.2f}\")\n",
        "        # Prefer higher silhouette, then higher CH, then lower DB\n",
        "        score = metrics[\"silhouette\"]\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "            best_labels = labels_k\n",
        "\n",
        "    print(f\"Selected best k={best_k} with silhouette={best_score:.4f}\")\n",
        "\n",
        "    # Save final labels\n",
        "    df_original[\"ensemble_cluster\"] = best_labels\n",
        "    df_original.to_csv(os.path.join(OUTPUT_DIR, \"data_with_ensemble_clusters.csv\"), index=False)\n",
        "    print(\"Saved dataset with ensemble cluster labels\")\n",
        "\n",
        "    # Profile clusters\n",
        "    profile = profile_clusters(df_original, X_scaled_df, best_labels, \"ensemble\")\n",
        "    print(profile)\n",
        "\n",
        "    # Visualize final clusters\n",
        "    visualize_embedding(X_for_clustering, best_labels, f\"Ensemble Clusters (k={best_k})\", os.path.join(OUTPUT_DIR, \"ensemble_clusters_pca.png\"))\n",
        "    print(\"Saved PCA visualization\")\n",
        "\n",
        "    # Also save evaluation table\n",
        "    eval_df = pd.DataFrame([\n",
        "        {\n",
        "            \"k\": k,\n",
        "            \"silhouette\": m[\"silhouette\"],\n",
        "            \"davies_bouldin\": m[\"davies_bouldin\"],\n",
        "            \"calinski_harabasz\": m[\"calinski_harabasz\"],\n",
        "        }\n",
        "        for k, m in evaluations\n",
        "    ])\n",
        "    eval_df.to_csv(os.path.join(OUTPUT_DIR, \"ensemble_evaluation.csv\"), index=False)\n",
        "    print(\"Saved evaluation metrics for candidate ks\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm-Pg0jTSEmY",
        "outputId": "e8ffe643-a3cb-46db-d395-102da65c6901"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data with shape: (16207, 20)\n",
            "Generated 18 base clusterings\n",
            "Saved co-association matrix\n",
            "k=2: silhouette=0.5223, db=0.7676, ch=12159.63\n",
            "k=3: silhouette=0.3002, db=1.1809, ch=10561.92\n",
            "k=4: silhouette=0.2588, db=1.3063, ch=7953.49\n",
            "k=5: silhouette=0.2408, db=1.2113, ch=7807.46\n",
            "k=6: silhouette=0.1939, db=1.6056, ch=6155.98\n",
            "k=7: silhouette=0.1520, db=1.6666, ch=5279.50\n",
            "Selected best k=2 with silhouette=0.5223\n",
            "Saved dataset with ensemble cluster labels\n",
            "Saved cluster profiles to CSV\n",
            "   ensemble_cluster  count  mean_income  median_income  mean_expenses  \\\n",
            "0                 0  13994  4217.302203        4194.74    2538.574602   \n",
            "1                 1   2213  2828.627081        2712.87    1691.556783   \n",
            "\n",
            "    mean_savings  mean_dti  mean_savings_to_income  \n",
            "0  254478.874197  0.278577                5.034786  \n",
            "1  173428.378179  7.292951                5.091414  \n",
            "Saved PCA visualization\n",
            "Saved evaluation metrics for candidate ks\n"
          ]
        }
      ]
    }
  ]
}